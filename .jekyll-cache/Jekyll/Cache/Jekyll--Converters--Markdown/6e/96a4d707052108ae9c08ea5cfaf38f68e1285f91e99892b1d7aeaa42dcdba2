I"Z<h1 id="浅谈softmax">浅谈softmax</h1>

<p>Softmax Loss 因为其易于优化，收敛快等特性被广泛应用于图像分类领域。然而，直接使用 softmax loss 训练得到的 feature 拿到 retrieval，verification 等“需要设阈值”的任务时，往往并不够好（需要模型概率校准）。</p>

<p>这其中的原因还得从 Softmax 的本身的定义说起，Softmax loss 在形式上是 softmax 函数加上交叉熵损失，其中的指数操作，可以迅速放大原始的 logit 之间的差异，使得“正确类别概率接近于 1”的目标变得简单很多。这种效应可以称为“强者通吃”。同时它的目的是让所有的类别在概率空间具有最大的对数似然，也就是保证所有的类别都能分类正确，而 retievel 和 verification 任务所需要的是一个泛化性能更好的度量空间（metric space）。保证分类正确和保证一个泛化性优良的 metric space 这两者之间虽然相关性很强，但并不直接等价。</p>

<p>近年来，face recognition 领域的主要技术进展集中在如何改进 softmax 的 loss，使得既能充分利用其易于优化，收敛快的优良性质，又使得其能优化出一个具有优良泛化性的 metric 空间。而这些技术改进主要又能被归为两大类别，做归一化以及增大 margin。</p>

<h1 id="归一化">归一化</h1>

<p>下面的两种归一化常常结合在一起使用。</p>

<h2 id="weight-norm">weight norm</h2>

<p>神经网络最后的fc层中，每个类别的weight可以看做该类别的agent，其模长是和该类别下数据的数量成正比，这对于样本不均衡问题很不友好。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/WN-before.jpg" alt="" /></p>

<p>而经过权重归一化，该问题得到很大程度的改善。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/WN-after.jpg" alt="" /></p>

<h2 id="feature-norm">feature norm</h2>

<p>[1]中认为神经网络提取到的feature，往往简单样本feature的模长较大，而困难样本feature的模长较小，因此需要归一化。但[2]中认为一些困难样本实际上是质量不高的数据（离群点），feature norm 后反而会破坏样本原来的分布。除了归一化，还有论文认为样本应该特征进行白化，但由于均值很大，而方差又极小，在数值上白化基本上和 feature normalization 很接近。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/FN.jpg" alt="" /></p>

<p>大量实验发现 feature norm 后，网络难以收敛，因为此时特征向量都固定映射到半径为1的超球上，这会压缩特征表达的空间，解决办法是乘一个尺度因子s，相当于将超球的半径放大到s，超球变大，特征表达的空间自然也更大，从而得到更好的结果。其中尺度因子可以网络自己学习，但其实人为设定效果也不错。</p>

<h1 id="增大margin">增大margin</h1>

<p>由于FC层的操作与计算余弦相似度有联系，所以本质上经过FC层和 softmax loss 优化后的特征空间是按角度分布的，如下图所示。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/feature-distribution.jpg" alt="" /></p>

<p>因此可以加上一些角度上的margin，使得类内差异小，类间差异大。一方面softmax非线性地放大输出，减小训练难度，使分类问题更容易收敛；另一方面，下图中的损失函数都非线性减小了输出，增加训练难度，使训练得到的特征映射更好。这其中margin的度需要把握好，如果训练难度增加过大，那么网络会很难收敛，反倒效果不好。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/loss.jpg" alt="" /></p>

<p>[3]中说明其实增大margin的实质是让权值向量W和特征向量f之间的夹角更小，即同一类的所有特征向量f都向该类别的权值向量w聚拢，那么只有w分得较开的情况下，f才能更好地区分。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/margin.jpg" alt="" /></p>

<blockquote>
  <p>参考文献</p>

  <p>[1] <a href="https://zhuanlan.zhihu.com/p/76391405">人脸识别中Softmax-based Loss的演化史</a></p>

  <p>[2] <a href="https://zhuanlan.zhihu.com/p/33288325">深度挖坑：从数据角度看人脸识别中Feature Normalization,Weight Normalization以及Triplet的作用</a></p>

  <p>[3] <a href="https://zhuanlan.zhihu.com/p/34436551">人脸识别的LOSS（下）</a></p>
</blockquote>
:ET