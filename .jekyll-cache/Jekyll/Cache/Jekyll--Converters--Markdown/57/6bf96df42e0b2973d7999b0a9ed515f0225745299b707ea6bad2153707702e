I"<h1 id="浅谈softmax">浅谈softmax</h1>

<p>Softmax Loss 因为其易于优化，收敛快等特性被广泛应用于图像分类领域。然而，直接使用 softmax loss 训练得到的 feature 拿到 retrieval，verification 等“需要设阈值”的任务时，往往并不够好（需要模型概率校准）。</p>

<p>这其中的原因还得从 Softmax 的本身的定义说起，Softmax loss 在形式上是 softmax 函数加上交叉熵损失，其中的指数操作，可以迅速放大原始的 logit 之间的差异，使得“正确类别概率接近于 1”的目标变得简单很多。这种效应可以称为“强者通吃”。同时它的目的是让所有的类别在概率空间具有最大的对数似然，也就是保证所有的类别都能分类正确，而 retievel 和 verification 任务所需要的是一个泛化性能更好的度量空间（metric space）。保证分类正确和保证一个泛化性优良的 metric space 这两者之间虽然相关性很强，但并不直接等价。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/framework1.jpg" alt="" /></p>

<p>1、建立分级的Category Graph
由于imageNet中的class是分层的，在wordNet的基础上建立，如果把miniImageNet中的类别看做叶子结点，那么很容易向上得到父节点和祖先节点，从而构造出分级的Category Graph。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/category.jpg" alt="" /></p>

<p>2、计算各级原型点，并传播信息。
初始化原型点P0，子类、父类和祖先类都是求类中所有元素的平均值。
<img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/p0.jpg" alt="" /></p>

<p>接着利用注意力机制，计算所有父节传播给子节点的信息P+。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/p+.jpg" alt="" /></p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/att.jpg" alt="" /></p>

<p>最后将P0与P+融合，得到最终子类更新后的原型表示P。（父类与祖先类不更新）</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/p.jpg" alt="" /></p>

<p>注意该方法最终需要优化的参数只有cnn和att两个模块。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/algorithm.jpg" alt="" /></p>

<p>3、PPN和PPN+
train时父类与祖先类原型的信息在test时是可获取的.</p>

<p>PPN+:test时知道具体的父类祖先类，若父类祖先类没有出现在train中，则对该类中所有样本求平均得到P。</p>

<p>PPN:test时不知道具体的父类祖先类，根据KNN选出最近的K个父类祖先类，从而进行信息传播。</p>

<p>4、实验结果</p>

<p>本文中的Category Graph也可用于protoNet和GNN等，但效果提升不大(不知道具体怎么应用的，只是简单的加上父类祖先类标签进行分类吗？)</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/exp.jpg" alt="" /></p>

<p>5、future work</p>

<p>（1）本文中只借助att用了一步信息传播，可不可以实现多步。</p>

<p>（2）本文构建的是Category Graph，可不可以构建Attribute Graph甚至其他可迁移的有用信息的graph。</p>

<h1 id="large-scale-few-shot-learning-knowledge-transfer-with-class-hierarchy">Large-Scale Few-Shot Learning Knowledge Transfer With Class Hierarchy</h1>

<p>本文主要是在特征提取上作文章，最后的分类借助protoNet中的metric方法，其实还可以用其他metric的方法。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/framework2.jpg" alt="" /></p>

<p>1、Class Hierarchy</p>

<p>minImageNet中的每个类名称在glove2vec中都可以找到，因此借助glove2vec对类名称进行K-means的聚类。文中实验过不同的分层聚类数，最红发现分三层，分别为底层叶子结点200类，中间层40类和顶层8类，效果最好。</p>

<p>本文构建Class Hierarchy使用了target classes，但其实不使用target，也能达到不错的效果。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/k-means.jpg" alt="" /></p>

<p>2、特征提取模块</p>

<p>train时使用传统的训练策略，而最终test时，只用其中的CNN部分。</p>

<p><img src="/img/in-post/post-paper-notes-PPN and Large-Scale Few-Shot Learning Knowledge Transfer With Class/feature.jpg" alt="" /></p>

<p>3、本文test时因为使用metric的方法，所以没有对CNN进行微调，其实可以用传统训练法，借助supportset进行微调，不知效果如何。但也有可能正是因为固定住CNN，才使得train时的knowledge得以很好地transfer。</p>

<p>#总结</p>

<p>这两篇文章想法有共通之处，都借助了父类祖先类（超类）使网络学习到的知识更好地迁移到没有见过的新类上，那么这种超类的思想能否与GNN相结合呢？</p>
:ET