I"<h1 id="浅谈softmax">浅谈softmax</h1>

<p>Softmax Loss 因为其易于优化，收敛快等特性被广泛应用于图像分类领域。然而，直接使用 softmax loss 训练得到的 feature 拿到 retrieval，verification 等“需要设阈值”的任务时，往往并不够好（需要模型概率校准）。</p>

<p>这其中的原因还得从 Softmax 的本身的定义说起，Softmax loss 在形式上是 softmax 函数加上交叉熵损失，其中的指数操作，可以迅速放大原始的 logit 之间的差异，使得“正确类别概率接近于 1”的目标变得简单很多。这种效应可以称为“强者通吃”。同时它的目的是让所有的类别在概率空间具有最大的对数似然，也就是保证所有的类别都能分类正确，而 retievel 和 verification 任务所需要的是一个泛化性能更好的度量空间（metric space）。保证分类正确和保证一个泛化性优良的 metric space 这两者之间虽然相关性很强，但并不直接等价。</p>

<p>近年来，face recognition 领域的主要技术进展集中在如何改进 softmax 的 loss，使得既能充分利用其易于优化，收敛快的优良性质，又使得其能优化出一个具有优良泛化性的 metric 空间。而这些技术改进主要又能被归为两大类别，做归一化以及加 margin。</p>

<h1 id="归一化">归一化</h1>
<h2 id="weight-norm">weight norm</h2>

<p>神经网络最后的fc层中，每个类别的weight可以看做该类别的agent，其模长是和该类别下数据的数量成正比，这对于样本不均衡问题很不友好。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/WN-before.jpg" alt="" /></p>

<p>而经过权重归一化，该问题得到很大程度的改善。</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/WN-after.jpg" alt="" /></p>

<h2 id="feature-norm">feature norm</h2>

<p>[1]中认为神经网络提取到的feature，往往简单样本feature的模长较大，而困难样本feature的模长较小，因此需要归一化。但[2]中认为一些困难样本实际上是质量不高的数据</p>

<p><img src="/img/in-post/post-paper-notes-FN WN and loss in face recognition/FN.jpg" alt="" /></p>
:ET